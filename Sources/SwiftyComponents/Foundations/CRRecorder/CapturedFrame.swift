//
//  CapturedFrame.swift
//  CoreRecorder
//
//  Created by lixindong on 2025/6/6.
//


/*
See the LICENSE.txt file for this sample's licensing information.

Abstract:
An object that captures a stream of captured sample buffers containing screen and audio content.
*/
import Foundation
import AVFAudio
@preconcurrency import ScreenCaptureKit
import OSLog
import Combine

/// A structure that contains the video data to render.
struct CapturedFrame: Sendable {
    static let invalid = CapturedFrame(surface: nil, contentRect: .zero, contentScale: 0, scaleFactor: 0)

    nonisolated(unsafe)
    let surface: IOSurface?
    
    let contentRect: CGRect
    let contentScale: CGFloat
    let scaleFactor: CGFloat
    var size: CGSize { contentRect.size }
}

/// An object that wraps an instance of `SCStream`, and returns its results as an `AsyncThrowingStream`.
@preconcurrency
class CaptureEngine: NSObject, @unchecked Sendable, @preconcurrency SCStreamDelegate {
    
    private let logger = Logger()

    private(set) var stream: SCStream?
    private var streamOutput: CaptureEngineStreamOutput?
    private let videoSampleBufferQueue = DispatchQueue(label: "com.example.apple-samplecode.VideoSampleBufferQueue")
    private let audioSampleBufferQueue = DispatchQueue(label: "com.example.apple-samplecode.AudioSampleBufferQueue")
    
    // Performs average and peak power calculations on the audio samples.
    private let powerMeter = PowerMeter()
    var audioLevels: AudioLevels { powerMeter.levels }
    
    var captureFileOutput: SingleCaptureFileOutput? = nil
    
    // Store the the startCapture continuation, so that you can cancel it when you call stopCapture().
    private var continuation: AsyncThrowingStream<CapturedFrame, Error>.Continuation?
    
    // ç¬¬ä¸€å¸§æ ‡è®°
    private var hasReceivedFirstFrame = false
    var firstFrameTimestampHandler: ((CFAbsoluteTime) -> Void)?
    
    // MARK: - é¢„è§ˆå›è°ƒ
    /// è§†é¢‘å¸§é¢„è§ˆå›è°ƒ
    var videoFramePreviewHandler: ((CapturedFrame) -> Void)?
    /// éŸ³é¢‘ç”µå¹³é¢„è§ˆå›è°ƒ  
    var audioLevelPreviewHandler: (() -> Void)?
    /// é”™è¯¯å¤„ç†å›è°ƒ
    var errorHandler: ((Error) -> Void)?

    // é¿å…é‡å¤æ”¶å°¾çš„æ ‡è®°ï¼ˆç³»ç»Ÿåœæ­¢/æ‰‹åŠ¨åœæ­¢ç­‰åœºæ™¯ï¼‰
    private var hasFinalized = false
    
    /// - Tag: StartCapture
    func startCapture(configuration: SCStreamConfiguration, filter: SCContentFilter) -> AsyncThrowingStream<CapturedFrame, Error> {
        AsyncThrowingStream<CapturedFrame, Error> { continuation in
            // The stream output object. Avoid reassigning it to a new object every time startCapture is called.
            let streamOutput = CaptureEngineStreamOutput(continuation: continuation, captureFileOutput: captureFileOutput, firstFrameTimestampHandler: firstFrameTimestampHandler)
            self.streamOutput = streamOutput
            streamOutput.capturedFrameHandler = { continuation.yield($0) }
            streamOutput.pcmBufferHandler = { self.powerMeter.process(buffer: $0) }

            // æ·»åŠ é¢„è§ˆå›è°ƒ
            streamOutput.videoFramePreviewHandler = { [weak self] frame in
                self?.videoFramePreviewHandler?(frame)
            }
            streamOutput.audioLevelPreviewHandler = { [weak self] in
                self?.audioLevelPreviewHandler?()
            }
            self.continuation = continuation

            do {
                stream = SCStream(filter: filter, configuration: configuration, delegate: streamOutput)
                
                // Add a stream output to capture screen content.
                try stream?.addStreamOutput(streamOutput, type: .screen, sampleHandlerQueue: videoSampleBufferQueue)
                try stream?.addStreamOutput(streamOutput, type: .audio, sampleHandlerQueue: audioSampleBufferQueue)
                stream?.startCapture()
                self.captureFileOutput?.startSession()
            } catch {
                print("âŒ Failed to start capture: \(error)")
                continuation.finish(throwing: error)
            }
        }
    }
    
    /// - Tag: StartCapture
    func startCaptureDirectly(configuration: SCStreamConfiguration, filter: SCContentFilter) throws {
        // The stream output object. Avoid reassigning it to a new object every time startCapture is called.
        let streamOutput = CaptureEngineStreamOutput(continuation: continuation, captureFileOutput: captureFileOutput, firstFrameTimestampHandler: firstFrameTimestampHandler)
        self.streamOutput = streamOutput
        streamOutput.capturedFrameHandler = { _ in }
        streamOutput.pcmBufferHandler = { self.powerMeter.process(buffer: $0) }
        
        // æ·»åŠ é¢„è§ˆå›è°ƒ
        streamOutput.videoFramePreviewHandler = { [weak self] frame in
            self?.videoFramePreviewHandler?(frame)
        }
        streamOutput.audioLevelPreviewHandler = { [weak self] in
            self?.audioLevelPreviewHandler?()
        }
        streamOutput.onError = { error in
            NSLog("ğŸŒŠ [STREAM_OUTPUT_DIRECT_ERROR] ç›´æ¥æ¨¡å¼æµè¾“å‡ºé”™è¯¯: %@", error.localizedDescription)
            let nsError = error as NSError
            Task { [weak self] in
                // å¦‚æœç³»ç»Ÿå·²åœæ­¢æµ (-3821)ï¼Œä¸è¦å†æ¬¡è°ƒç”¨ stopCapture()
                if nsError.domain == "com.apple.ScreenCaptureKit.SCStreamErrorDomain" && nsError.code == -3821 {
                    await self?.finalizeAfterExternalStop()
                } else {
                    try await self?.stopCapture()
                }
                self?.errorHandler?(error)
            }
            RecorderDiagnostics.shared.recordError(error)
        }
        captureFileOutput?.onError = { error in
            NSLog("ğŸ“ [CAPTURE_FILE_ERROR] æ–‡ä»¶è¾“å‡ºé”™è¯¯: %@", error.localizedDescription)
            Task { [weak self]  in
                try await self?.stopCapture()
                self?.errorHandler?(error)
            }
        }
        
        stream = SCStream(filter: filter, configuration: configuration, delegate: streamOutput)
        RecorderDiagnostics.shared.onStartCapture(configuration: configuration)
        
        // Add a stream output to capture screen content.
        try stream?.addStreamOutput(streamOutput, type: .screen, sampleHandlerQueue: videoSampleBufferQueue)
        try stream?.addStreamOutput(streamOutput, type: .audio, sampleHandlerQueue: audioSampleBufferQueue)
        stream?.startCapture()
        self.captureFileOutput?.startSession()
    }

    /// å½“ç³»ç»Ÿå·²åœæ­¢ SCStreamï¼ˆå¦‚ -3821ï¼‰æ—¶çš„æ”¶å°¾é€»è¾‘ï¼š
    /// è·³è¿‡å¯¹ SCStream çš„ stopCapture è°ƒç”¨ï¼Œä»…åšæœ¬åœ°æ¸…ç†ä¸æ–‡ä»¶å…³é—­ã€‚
    func finalizeAfterExternalStop() async {
        if hasFinalized { return }
        hasFinalized = true
        powerMeter.processSilence()
        do {
            try await Task.sleep(for: .seconds(1))
            try await captureFileOutput?.stopSession()
        } catch {
            NSLog("âš ï¸ [FINALIZE_AFTER_EXTERNAL_STOP] å…³é—­æ–‡ä»¶å†™å…¥æ—¶å‡ºé”™: %@", (error as NSError).localizedDescription)
        }
        // é¿å…åç»­é‡å¤ stop è°ƒç”¨è§¦å‘ -3808 æ—¥å¿—
        stream = nil
        RecorderDiagnostics.shared.onStopCapture()
    }
    
//    func stream(_ stream: SCStream, didStopWithError error: any Error) {
//        print("âŒ Failed to didStopWithError: \(error)")
//        Task {
//            await safeStopCaptureFileOutput()
//            // æ£€æµ‹å†…å­˜ä¸è¶³é”™è¯¯
//            if let nsError = error as NSError? {
//                let isMemoryError = checkMemoryError(nsError)
//                if isMemoryError {
//                    print("âš ï¸ æ£€æµ‹åˆ°å†…å­˜ä¸è¶³é—®é¢˜ï¼Œè¯·å…³é—­å…¶ä»–åº”ç”¨æˆ–é™ä½å½•åˆ¶è´¨é‡")
//                    // å¯ä»¥é€šè¿‡é€šçŸ¥ç­‰æ–¹å¼æé†’ç”¨æˆ·
//                    NotificationCenter.default.post(
//                        name: NSNotification.Name("CaptureMemoryWarning"),
//                        object: nil,
//                        userInfo: ["error": error, "suggestion": "å†…å­˜ä¸è¶³ï¼Œå»ºè®®å…³é—­å…¶ä»–åº”ç”¨æˆ–é™ä½å½•åˆ¶è´¨é‡"]
//                    )
//                }
//            }
//            
//            // è°ƒç”¨é”™è¯¯å¤„ç†å›è°ƒ
////            errorHandler?(CRRecordingError.streamError(error))
//            
//            continuation?.finish(throwing: error)
//        }
//    }
    
    func stopCapture() async throws {
        if hasFinalized { return }
        hasFinalized = true
        do {
            try await stream?.stopCapture()
            continuation?.finish()
        } catch {
            continuation?.finish(throwing: error)
        }
        // æ ‡è®°æµå·²æ— æ•ˆï¼Œé¿å…åç»­é‡å¤ stop è°ƒç”¨
        stream = nil
        powerMeter.processSilence()
        try await Task.sleep(for: .seconds(1))
        try await captureFileOutput?.stopSession()
        RecorderDiagnostics.shared.onStopCapture()
    }
  
    /// - Tag: UpdateStreamConfiguration
    func update(configuration: SCStreamConfiguration, filter: SCContentFilter) async {
        do {
            try await stream?.updateConfiguration(configuration)
            try await stream?.updateContentFilter(filter)
        } catch {
            logger.error("Failed to update the stream session: \(String(describing: error))")
        }
    }
    
    // MARK: - ç§æœ‰è¾…åŠ©æ–¹æ³•
    
    /// æ£€æµ‹æ˜¯å¦ä¸ºå†…å­˜ç›¸å…³é”™è¯¯
    private func checkMemoryError(_ error: NSError) -> Bool {
        // æ£€æŸ¥é”™è¯¯æè¿°ä¸­çš„å†…å­˜ç›¸å…³å…³é”®è¯
        let errorDescription = error.localizedDescription.lowercased()
        let memoryKeywords = [
            "memory", "insufficient", "low memory", "out of memory",
            "å†…å­˜ä¸è¶³", "å†…å­˜", "insufficient memory"
        ]
        
        for keyword in memoryKeywords {
            if errorDescription.contains(keyword) {
                return true
            }
        }
        
        // æ£€æŸ¥å…·ä½“çš„é”™è¯¯ä»£ç 
        switch error.code {
        case -6728, // Memory allocation failed
             -12903, // Insufficient memory
             -34018: // Memory pressure
            return true
        default:
            break
        }
        
        // æ£€æŸ¥ç³»ç»Ÿå†…å­˜å‹åŠ›
        return ProcessInfo.processInfo.thermalState == .critical ||
               isSystemUnderMemoryPressure()
    }
    
    /// æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦å¤„äºå†…å­˜å‹åŠ›çŠ¶æ€
    private func isSystemUnderMemoryPressure() -> Bool {
        var info = vm_statistics64()
        var count = mach_msg_type_number_t(MemoryLayout<vm_statistics64>.size / MemoryLayout<integer_t>.size)
        
        let result = withUnsafeMutablePointer(to: &info) {
            $0.withMemoryRebound(to: integer_t.self, capacity: Int(count)) {
                host_statistics64(mach_host_self(), HOST_VM_INFO64, $0, &count)
            }
        }
        
        if result == KERN_SUCCESS {
            let totalMemory = ProcessInfo.processInfo.physicalMemory
            let pageSize = UInt64(sysconf(_SC_PAGESIZE))  // ä½¿ç”¨ sysconf è·å–é¡µé¢å¤§å°ï¼Œçº¿ç¨‹å®‰å…¨
            let freeMemory = UInt64(info.free_count) * pageSize
            let memoryUsageRatio = Double(totalMemory - freeMemory) / Double(totalMemory)
            
            // å¦‚æœå†…å­˜ä½¿ç”¨ç‡è¶…è¿‡ 90%ï¼Œè®¤ä¸ºæ˜¯å†…å­˜å‹åŠ›çŠ¶æ€
            return memoryUsageRatio > 0.9
        }
        
        return false
    }
    
    /// å®‰å…¨åœæ­¢æ–‡ä»¶è¾“å‡º
    private func safeStopCaptureFileOutput() async {
        guard let fileOutput = captureFileOutput else { return }
        
        do {
            print("ğŸ›¡ï¸ æ­£åœ¨å®‰å…¨å…³é—­å½•åˆ¶æ–‡ä»¶å†™å…¥...")
            try await fileOutput.stopSession()
            print("âœ… å½•åˆ¶æ–‡ä»¶å·²å®‰å…¨å…³é—­")
        } catch {
            print("âš ï¸ å…³é—­å½•åˆ¶æ–‡ä»¶æ—¶å‡ºé”™: \(error.localizedDescription)")
            // å³ä½¿å‡ºé”™ä¹Ÿè¦å°è¯•æ¸…ç†èµ„æº
        }
    }
}

/// A class that handles output from an SCStream, and handles stream errors.
@preconcurrency
private class CaptureEngineStreamOutput: NSObject, SCStreamOutput, @preconcurrency SCStreamDelegate {
    
    var pcmBufferHandler: ((AVAudioPCMBuffer) -> Void)?
    var capturedFrameHandler: ((CapturedFrame) -> Void)?
    
    // Store the  startCapture continuation, so you can cancel it if an error occurs.
    private var continuation: AsyncThrowingStream<CapturedFrame, Error>.Continuation?
    
    var captureFileOutput: CaptureFileOutput?
    
    // ç¬¬ä¸€å¸§æ ‡è®°
    private var hasReceivedFirstFrame = false
    var firstFrameTimestampHandler: ((CFAbsoluteTime) -> Void)?
    
    private var hasReceivedFirstAudio = false
    
    // MARK: - é¢„è§ˆå›è°ƒ
    /// è§†é¢‘å¸§é¢„è§ˆå›è°ƒ
    var videoFramePreviewHandler: ((CapturedFrame) -> Void)?
    /// éŸ³é¢‘ç”µå¹³é¢„è§ˆå›è°ƒ  
    var audioLevelPreviewHandler: (() -> Void)?
    
    var onError: (Error) -> Void = {_ in}
    
    init(continuation: AsyncThrowingStream<CapturedFrame, Error>.Continuation?, captureFileOutput: CaptureFileOutput?, firstFrameTimestampHandler: ((CFAbsoluteTime) -> Void)? = nil) {
        self.continuation = continuation
        self.captureFileOutput = captureFileOutput
        self.firstFrameTimestampHandler = firstFrameTimestampHandler
    }
    
    /// - Tag: DidOutputSampleBuffer
    func stream(_ stream: SCStream, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, of outputType: SCStreamOutputType) {
        
        // Return early if the sample buffer is invalid.
        guard sampleBuffer.isValid else { return }
        
        // Determine which type of data the sample buffer contains.
        switch outputType {
        case .screen:
            // Create a CapturedFrame structure for a video sample buffer.
            guard let frame = createFrame(for: sampleBuffer) else { return }
            RecorderDiagnostics.shared.onCaptureVideoFrame()
            
            // æ•è·ç¬¬ä¸€å¸§çš„æ—¶é—´æˆ³
            if !hasReceivedFirstFrame {
                hasReceivedFirstFrame = true
                let firstFrameTimestamp = CFAbsoluteTimeGetCurrent()
                firstFrameTimestampHandler?(sampleBuffer.presentationTimeStamp.seconds)
                print("[record-time] å¼€å§‹æ—¶é—´: ç³»ç»Ÿç”»é¢ \(CFAbsoluteTimeGetCurrent()) time \(sampleBuffer.presentationTimeStamp.seconds)")
            }
            
            capturedFrameHandler?(frame)
            captureFileOutput?.saveFrame(for: sampleBuffer)

            // è°ƒç”¨è§†é¢‘å¸§é¢„è§ˆå›è°ƒ
            videoFramePreviewHandler?(frame)
            RecorderDiagnostics.shared.onVideoSample(size: frame.size)
            
        case .audio:
            // Process audio as an AVAudioPCMBuffer for level calculation.
            if !hasReceivedFirstFrame {
                print("[record-time] å¼€å§‹æ—¶é—´: ç³»ç»Ÿå£°éŸ³ \(CFAbsoluteTimeGetCurrent())(date\(Date().timeIntervalSince1970) time\(sampleBuffer.presentationTimeStamp.seconds)")
            }
            RecorderDiagnostics.shared.onCaptureAudioSample()
            handleAudio(for: sampleBuffer)
            captureFileOutput?.saveAudio(for: sampleBuffer)
            
            // è°ƒç”¨éŸ³é¢‘ç”µå¹³é¢„è§ˆå›è°ƒ
            audioLevelPreviewHandler?()
            
        @unknown default:
            fatalError("Encountered unknown stream output type: \(outputType)")
        }
    }
    
    /// Create a `CapturedFrame` for the video sample buffer.
    private func createFrame(for sampleBuffer: CMSampleBuffer) -> CapturedFrame? {
        
        // Retrieve the array of metadata attachments from the sample buffer.
        guard let attachmentsArray = CMSampleBufferGetSampleAttachmentsArray(sampleBuffer,
                                                                             createIfNecessary: false) as? [[SCStreamFrameInfo: Any]],
              let attachments = attachmentsArray.first else { return nil }
        
        // Validate the status of the frame. If it isn't `.complete`, return nil.
        guard let statusRawValue = attachments[SCStreamFrameInfo.status] as? Int,
              let status = SCFrameStatus(rawValue: statusRawValue),
              status == .complete else { return nil }
        
        // Get the pixel buffer that contains the image data.
        guard let pixelBuffer = sampleBuffer.imageBuffer else { return nil }
        
        // Get the backing IOSurface.
        guard let surfaceRef = CVPixelBufferGetIOSurface(pixelBuffer)?.takeUnretainedValue() else { return nil }
        let surface = unsafeBitCast(surfaceRef, to: IOSurface.self)
        
        // Retrieve the content rectangle, scale, and scale factor.
        guard let contentRectDict = attachments[.contentRect],
              let contentRect = CGRect(dictionaryRepresentation: contentRectDict as! CFDictionary),
              let contentScale = attachments[.contentScale] as? CGFloat,
              let scaleFactor = attachments[.scaleFactor] as? CGFloat else { return nil }
        
        // Create a new frame with the relevant data.
        let frame = CapturedFrame(surface: surface,
                                  contentRect: contentRect,
                                  contentScale: contentScale,
                                  scaleFactor: scaleFactor)
        return frame
    }
    
    private func handleAudio(for buffer: CMSampleBuffer) -> Void? {
        // Create an AVAudioPCMBuffer from an audio sample buffer.
        try? buffer.withAudioBufferList { audioBufferList, blockBuffer in
            guard let description = buffer.formatDescription?.audioStreamBasicDescription,
                  let format = AVAudioFormat(standardFormatWithSampleRate: description.mSampleRate, channels: description.mChannelsPerFrame),
                  let samples = AVAudioPCMBuffer(pcmFormat: format, bufferListNoCopy: audioBufferList.unsafePointer)
            else { return }
            pcmBufferHandler?(samples)
        }
    }
    
    func stream(_ stream: SCStream, didStopWithError error: Error) {
        NSLog("ğŸ’¢ [STREAM_OUTPUT_ERROR] CaptureEngineStreamOutput æ£€æµ‹åˆ°æµé”™è¯¯: %@", error.localizedDescription)
        print("âŒ CaptureEngineStreamOutput - æµå‡ºé”™: \(error.localizedDescription)")
        // è·å–æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        let nsError = error as NSError
        NSLog("é”™è¯¯åŸŸ: %@", nsError.domain)
        NSLog("é”™è¯¯ä»£ç : %ld", nsError.code)
        NSLog("ç”¨æˆ·ä¿¡æ¯: %@", nsError.userInfo)
        NSLog("åº•å±‚é”™è¯¯: %@", nsError.underlyingErrors.map({ $0.localizedDescription }) ?? "æ— ")
        print("é”™è¯¯å¯¹è±¡", error)

        Task {
            await self.onError(error)
        }
        continuation?.finish(throwing: error)
        RecorderDiagnostics.shared.recordError(error)
    }

    func streamDidBecomeActive(_ stream: SCStream) {
        RecorderDiagnostics.shared.onStreamDidBecomeActive()
    }

    func streamDidBecomeInactive(_ stream: SCStream) {
        RecorderDiagnostics.shared.onStreamDidBecomeInactive()
    }
    
    /// å®‰å…¨åœæ­¢æ–‡ä»¶è¾“å‡º
    private func safeStopCaptureFileOutput() async {
        guard let fileOutput = captureFileOutput else { return }
        
        do {
            print("ğŸ›¡ï¸ æ­£åœ¨å®‰å…¨å…³é—­å½•åˆ¶æ–‡ä»¶å†™å…¥...")
            try await fileOutput.stopSession()
            print("âœ… å½•åˆ¶æ–‡ä»¶å·²å®‰å…¨å…³é—­")
        } catch {
            print("âš ï¸ å…³é—­å½•åˆ¶æ–‡ä»¶æ—¶å‡ºé”™: \(error.localizedDescription)")
            // å³ä½¿å‡ºé”™ä¹Ÿè¦å°è¯•æ¸…ç†èµ„æº
        }
    }
}
